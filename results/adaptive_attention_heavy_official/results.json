{
  "experiment_name": "adaptive_attention_heavy_official",
  "model_name": "microsoft/DialoGPT-medium",
  "dataset_name": "tatsu-lab/alpaca",
  "dataset_size": 200,
  "strategy": "attention_heavy",
  "base_rank": 16,
  "min_rank": 4,
  "max_rank": 32,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "learning_rate": 0.0003,
  "num_epochs": 3,
  "batch_size": 8,
  "seed": 42,
  "training_time_seconds": 90.210599,
  "final_train_loss": 0,
  "final_eval_loss": NaN,
  "final_perplexity": NaN,
  "trainable_parameters": 5111808,
  "total_parameters": 359934976,
  "parameter_efficiency": 0.014202031869222956,
  "allocation_info": {
    "strategy": "attention_heavy",
    "rank_dict": {
      "c_attn": [
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16
      ],
      "c_proj": [
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16,
        16
      ],
      "c_fc": [
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8,
        8
      ]
    }
  },
  "timestamp": "2025-07-07T21:07:39.696879"
}